We basically perform the following functions :
![](https://hackmd.io/_uploads/S1m5ZwIp2.png)

FOr working with multiple trainig examples we follow the following notation : 
![](https://hackmd.io/_uploads/rycszPITh.png)

Sigmoid function should be used for binary classifications like 0 or 1

We generally prefer ReLU function

Types of activation functions : 

![](https://hackmd.io/_uploads/S1bBVuLTh.png)

tanh works better for hidden layers than sigmoid functions

#Derivatives of activation functions : 
sigmoid 
![](https://hackmd.io/_uploads/Sy1_0_ITh.png)

tanh : 
![](https://hackmd.io/_uploads/BJNjRuUpn.png)

![](https://hackmd.io/_uploads/S1BhAuU62.png)

![](https://hackmd.io/_uploads/SJFa0_UTn.png)


![](https://hackmd.io/_uploads/S1kX1FIa2.png)



# DEEP NEURAL NETWORKS


![](https://hackmd.io/_uploads/r1Om-s_Tn.png)

Generalised notation 
![](https://hackmd.io/_uploads/SyIuyn_6h.png)

We can generalise matrix dimensions as follwos : 
![](https://hackmd.io/_uploads/BkUZZ3OT3.png)

![](https://hackmd.io/_uploads/HkvPw2uT3.png)

![](https://hackmd.io/_uploads/BJZJY2_6n.png)










