{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJmT0RTBse1mTfvnjOxkJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanNanda17/GestureSense/blob/Mihir/MNIST%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AMi_ySft4JFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "4ae316e8-bea7-4887-c5b7-eaedbf4ce0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8862d48bebcc>\u001b[0m in \u001b[0;36m<cell line: 180>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: continuous-multioutput is not supported"
          ]
        }
      ],
      "source": [
        "#\n",
        "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
        "#\n",
        "import numpy as np # linear algebra\n",
        "import struct\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "from array import array\n",
        "from os.path  import join\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "x,y = mnist['data'],mnist['target']\n",
        "\n",
        "x_train,x_test = x[:60000], x[60000:]\n",
        "y_train,y_test = y[:60000], y[60000:]\n",
        "\n",
        "x_train = np.array(x_train,dtype=float)\n",
        "x_test = np.array(x_test,dtype = float)\n",
        "y_train = np.array(y_train,dtype = float)\n",
        "y_test = np.array(y_test,dtype=float)\n",
        "\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "y_test = y_test/10\n",
        "y_train = y_train/10\n",
        "\n",
        "y_train = y_train.reshape(60000,1)\n",
        "y_test = y_test.reshape(10000,1)\n",
        "\n",
        "x_train = x_train.T\n",
        "x_test = x_test.T\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "\n",
        "def relu(x):\n",
        "  y = np.maximum(0,x)\n",
        "  return y\n",
        "\n",
        "def sigmoid(x):\n",
        "  y = 1/(1+np.exp(-x))\n",
        "  return y\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "\n",
        "\n",
        "    W1 = np.random.randn(n_h,n_x)*0.01\n",
        "    b1 = np.zeros((n_h,1),dtype= float)\n",
        "    W2 = np.random.randn(n_y,n_h)*0.01\n",
        "    b2 = np.zeros((n_y,1),dtype = float)\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W,A) + b\n",
        "\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "\n",
        "        Z,cache = linear_forward(A_prev,W,b)\n",
        "        A = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "\n",
        "        Z,cache = linear_forward(A_prev,W,b)\n",
        "        A = relu(Z)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[0]\n",
        "    cost = -np.sum(np.dot(Y,np.log(AL).T) + np.dot((1-Y),np.log(1-AL).T))/m\n",
        "\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "\n",
        "    dW = np.dot(dZ,A_prev.T)/m\n",
        "    db = np.sum((dZ),axis=1,keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    if activation == \"relu\":\n",
        "\n",
        "        dZ = relu(dA)\n",
        "        dA_prev,dW , db = linear_backward(dZ,cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "\n",
        "        dZ = sigmoid(dA)\n",
        "        dA_prev,dW,db = linear_backward(dZ,cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def update_parameters(params, grads, learning_rate):\n",
        "\n",
        "    parameters = params.copy()\n",
        "    parameters[\"W\"+str(1)] = parameters[\"W\"+str(1)] - learning_rate*grads[\"dW\" + str(1)]\n",
        "    parameters[\"b\"+str(1)] = parameters[\"b\"+str(1)] - learning_rate*grads[\"db\" + str(1)]\n",
        "    parameters[\"W\"+str(2)] = parameters[\"W\"+str(2)] - learning_rate*grads[\"dW\" + str(2)]\n",
        "    parameters[\"b\"+str(2)] = parameters[\"b\"+str(2)] - learning_rate*grads[\"db\" + str(2)]\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def two_layer_model(X, Y, layers_dims, learning_rate, num_iterations):\n",
        "\n",
        "    grads = {}\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "\n",
        "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
        "\n",
        "\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        A1,cache1 = linear_activation_forward(X,W1,b1,\"relu\")\n",
        "        A2,cache2 = linear_activation_forward(A1,W2,b2,\"sigmoid\")\n",
        "\n",
        "        cost = compute_cost(A2,Y)\n",
        "\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "\n",
        "\n",
        "        dA1,dW2,db2 = linear_activation_backward(dA2,cache2,\"sigmoid\")\n",
        "        dA0,dW1,db1 = linear_activation_backward(dA1,cache1,\"relu\")\n",
        "\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "\n",
        "\n",
        "        parameters = update_parameters(parameters,grads,learning_rate)\n",
        "\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "\n",
        "    return parameters, costs,A2\n",
        "\n",
        "parameters, costs,A2 = two_layer_model(x_train, y_train, (784, 10, 1), 0.01, 250)\n",
        "W1 = parameters[\"W1\"]\n",
        "b1 = parameters[\"b1\"]\n",
        "W2 = parameters[\"W2\"]\n",
        "b2 = parameters[\"b2\"]\n",
        "A1,cache1 = linear_activation_forward(x_train,W1,b1,\"relu\")\n",
        "A2,cache2 = linear_activation_forward(A1,W2,b2,\"sigmoid\")\n",
        "acc = accuracy_score(y_train,A2)\n",
        "print(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}